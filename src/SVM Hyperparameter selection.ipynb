{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wj/2bdfjxmx6_dd5gvnsyg6_rvc0000gn/T/ipykernel_2914/1438416688.py:98: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(infile, 'rU') as fid :\n",
      "/var/folders/wj/2bdfjxmx6_dd5gvnsyg6_rvc0000gn/T/ipykernel_2914/1438416688.py:133: DeprecationWarning: 'U' mode is deprecated\n",
      "  num_lines = sum(1 for line in open(infile,'rU'))\n",
      "/var/folders/wj/2bdfjxmx6_dd5gvnsyg6_rvc0000gn/T/ipykernel_2914/1438416688.py:137: DeprecationWarning: 'U' mode is deprecated\n",
      "  with open(infile, 'rU') as fid :\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVM Hyperparameter Selection based on accuracy:\n",
      "Linear SVM Hyperparameter Selection based on f1_score:\n",
      "Linear SVM Hyperparameter Selection based on auroc:\n",
      "Linear SVM Hyperparameter Selection based on precision:\n",
      "Linear SVM Hyperparameter Selection based on sensitivity:\n",
      "Linear SVM Hyperparameter Selection based on specificity:\n",
      "╒════════════╤════════════╤════════════╤═════════╤═════════════╤═══════════════╤═══════════════╕\n",
      "│            │   accuracy │   f1_score │   auroc │   precision │   sensitivity │   specificity │\n",
      "╞════════════╪════════════╪════════════╪═════════╪═════════════╪═══════════════╪═══════════════╡\n",
      "│ Best Score │      0.825 │   0.867256 │ 0.80301 │    0.860898 │         1     │      0.731714 │\n",
      "├────────────┼────────────┼────────────┼─────────┼─────────────┼───────────────┼───────────────┤\n",
      "│ Best C     │      1     │   1        │ 1       │    1        │         0.001 │      1        │\n",
      "╘════════════╧════════════╧════════════╧═════════╧═════════════╧═══════════════╧═══════════════╛\n",
      "From the result we can see that C=1 is the best for the model.\n",
      "\n",
      "part 3a: The gamma parameter defines how far the influence of a single training example reaches, \n",
      "with low values meaning ‘far’ and high values meaning ‘close’.\n",
      "\n",
      "part 3b: I check every possible combination of C and gamma.\n",
      "\n",
      "part 3c: for each metric, select optimal hyperparameter for RBF-SVM using CV\n",
      "RBF SVM Hyperparameter Selection based on accuracy:\n",
      "RBF SVM Hyperparameter Selection based on f1_score:\n",
      "RBF SVM Hyperparameter Selection based on auroc:\n",
      "RBF SVM Hyperparameter Selection based on precision:\n",
      "RBF SVM Hyperparameter Selection based on sensitivity:\n",
      "RBF SVM Hyperparameter Selection based on specificity:\n",
      "c_list ['Best c', 10, 10, 100, 100, 100, 100]\n",
      "g_list ['Gamma', 0.01, 0.01, 0.01, 0.01, 100, 0.01]\n",
      "s_list ['Score', 0.82, 0.87, 0.8, 0.85, 1.0, 0.72]\n",
      "╒════════╤════════════╤════════════╤═════════╤═════════════╤═══════════════╤═══════════════╕\n",
      "│        │   accuracy │   f1_score │   auroc │   precision │   sensitivity │   specificity │\n",
      "╞════════╪════════════╪════════════╪═════════╪═════════════╪═══════════════╪═══════════════╡\n",
      "│ Score  │       0.82 │       0.87 │    0.8  │        0.85 │             1 │          0.72 │\n",
      "├────────┼────────────┼────────────┼─────────┼─────────────┼───────────────┼───────────────┤\n",
      "│ Best c │      10    │      10    │  100    │      100    │           100 │        100    │\n",
      "├────────┼────────────┼────────────┼─────────┼─────────────┼───────────────┼───────────────┤\n",
      "│ Gamma  │       0.01 │       0.01 │    0.01 │        0.01 │           100 │          0.01 │\n",
      "╘════════╧════════════╧════════════╧═════════╧═════════════╧═══════════════╧═══════════════╛\n",
      "Compare with Linear-Kernel SVM, we can see that in RBF-kernel SVM method c = 10 are most likely\n",
      "to be the best, but in Linear-Kernel SVM c = 1 are the best.\n",
      "\n",
      "part 4a: train linear- and RBF-kernel SVMs with selected hyperparameters\n",
      "Based on the previous testing result, I choose c=1 for Linear Kernel SVM and c = 10, gamma = 0.01\n",
      "for the RBF-kernel method.\n",
      "\n",
      "part 4c: report performance on test data\n",
      "The performance of Linear-Kernel SVM is 0.9 which is good enough for this model.\n",
      "The performance of RBF model 0.9433962264150944 which is also good enough for this model. \n",
      "Base on the result, Linear-Kernel SVM is better than RBF model. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from string import punctuation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# !!! MAKE SURE TO USE SVC.decision_function(X), NOT SVC.predict(X) !!!\n",
    "# (this makes ``continuous-valued'' predictions)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "# self import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# functions -- input/output\n",
    "######################################################################\n",
    "\n",
    "def read_vector_file(fname):\n",
    "    \"\"\"\n",
    "    Reads and returns a vector from a file.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        fname  -- string, filename\n",
    "        \n",
    "    Returns\n",
    "    --------------------\n",
    "        labels -- numpy array of shape (n,)\n",
    "                    n is the number of non-blank lines in the text file\n",
    "    \"\"\"\n",
    "    return np.genfromtxt(fname)\n",
    "\n",
    "\n",
    "def write_label_answer(vec, outfile):\n",
    "    \"\"\"\n",
    "    Writes your label vector to the given file.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        vec     -- numpy array of shape (n,) or (n,1), predicted scores\n",
    "        outfile -- string, output filename\n",
    "    \"\"\"\n",
    "    \n",
    "    # for this project, you should predict 70 labels\n",
    "    if(vec.shape[0] != 70):\n",
    "        print(\"Error - output vector should have 70 rows.\")\n",
    "        print(\"Aborting write.\")\n",
    "        return\n",
    "    \n",
    "    np.savetxt(outfile, vec)    \n",
    "\n",
    "\n",
    "######################################################################\n",
    "# functions -- feature extraction\n",
    "######################################################################\n",
    "\n",
    "def extract_words(input_string):\n",
    "    \"\"\"\n",
    "    Processes the input_string, separating it into \"words\" based on the presence\n",
    "    of spaces, and separating punctuation marks into their own words.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        input_string -- string of characters\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        words        -- list of lowercase \"words\"\n",
    "    \"\"\"\n",
    "    \n",
    "    for c in punctuation :\n",
    "        input_string = input_string.replace(c, ' ' + c + ' ')\n",
    "    return input_string.lower().split()\n",
    "\n",
    "\n",
    "def extract_dictionary(infile):\n",
    "    \"\"\"\n",
    "    Given a filename, reads the text file and builds a dictionary of unique\n",
    "    words/punctuations.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile    -- string, filename\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        word_list -- dictionary, (key, value) pairs are (word, index)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_list = {}\n",
    "    with open(infile, 'rU') as fid :\n",
    "        ### ========== TODO : START ========== ###\n",
    "        # part 1a: process each line to populate word_list\n",
    "        lines = fid.readlines()\n",
    "        index = 0\n",
    "        for line in lines:   \n",
    "            words = extract_words(line)\n",
    "            for i in words:\n",
    "                if (i in word_list.keys()) == False:\n",
    "                    word_list[i] = index\n",
    "                    index = index + 1\n",
    "\n",
    "        ### ========== TODO : END ========== ###\n",
    "\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def extract_feature_vectors(infile, word_list):\n",
    "    \"\"\"\n",
    "    Produces a bag-of-words representation of a text file specified by the\n",
    "    filename infile based on the dictionary word_list.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        infile         -- string, filename\n",
    "        word_list      -- dictionary, (key, value) pairs are (word, index)\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        feature_matrix -- numpy array of shape (n,d)\n",
    "                          boolean (0,1) array indicating word presence in a string\n",
    "                            n is the number of non-blank lines in the text file\n",
    "                            d is the number of unique words in the text file\n",
    "    \"\"\"\n",
    "    \n",
    "    num_lines = sum(1 for line in open(infile,'rU'))\n",
    "    num_words = len(word_list)\n",
    "    feature_matrix = np.zeros((num_lines, num_words))\n",
    "    \n",
    "    with open(infile, 'rU') as fid :\n",
    "        ### ========== TODO : START ========== ###\n",
    "        # part 1b: process each line to populate feature_matrix\n",
    "\n",
    "        # j = 0\n",
    "        # for l in fid:\n",
    "        #     words = extract_words(l)\n",
    "        #     i = 0\n",
    "        #     for k in words:\n",
    "        #         if k in word_list:\n",
    "        #             feature_matrix[i, j] = 1\n",
    "        #         i = i + 1\n",
    "        #     j = j + 1\n",
    "\n",
    "        for index, line in enumerate(fid):\n",
    "\n",
    "            words_extracted = extract_words(line)\n",
    "\n",
    "            for i in words_extracted:\n",
    "                index_i = word_list[i]\n",
    "                feature_matrix[index,index_i] = 1\n",
    "\n",
    "\n",
    "\n",
    "        pass\n",
    "        ### ========== TODO : END ========== ###\n",
    "        \n",
    "    return feature_matrix\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# functions -- evaluation\n",
    "######################################################################\n",
    "\n",
    "def performance(y_true, y_pred, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Calculates the performance metric based on the agreement between the \n",
    "    true labels and the predicted labels.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        y_true -- numpy array of shape (n,), known labels\n",
    "        y_pred -- numpy array of shape (n,), (continuous-valued) predictions\n",
    "        metric -- string, option used to select the performance measure\n",
    "                  options: 'accuracy', 'f1-score', 'auroc', 'precision',\n",
    "                           'sensitivity', 'specificity'        \n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score  -- float, performance score\n",
    "    \"\"\"\n",
    "    # map continuous-valued predictions to binary labels\n",
    "    y_label = np.sign(y_pred)\n",
    "    y_label[y_label==0] = 1\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2a: compute classifier performance\n",
    "    # score = 0\n",
    "    if metric == \"accuracy\":\n",
    "        score = metrics.accuracy_score(y_true, y_label)\n",
    "    elif metric == \"f1_score\":\n",
    "        score = metrics.f1_score(y_true, y_label)\n",
    "    elif metric == \"auroc\":\n",
    "        score = metrics.roc_auc_score(y_true, y_label)\n",
    "    elif metric == \"precision\":\n",
    "        score = metrics.precision_score(y_true, y_label)\n",
    "\n",
    "    elif metric == \"sensitivity\":\n",
    "        conf_matrix = metrics.confusion_matrix(y_true, y_label)\n",
    "        score = conf_matrix[1,1]/float((conf_matrix[1,1]+conf_matrix[1, 0]))\n",
    "        #score = metrics.confusion_matrix(y_true, y_label)\n",
    "\n",
    "    elif metric == \"specificity\":\n",
    "        #score = metrics.confusion_matrix(y_true, y_label)\n",
    "        conf_matrix = metrics.confusion_matrix(y_true, y_label)\n",
    "        score = conf_matrix[0,0]/float((conf_matrix[0,0]+conf_matrix[0,1]))\n",
    "        \n",
    "    return score\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def cv_performance(clf, X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Splits the data, X and y, into k-folds and runs k-fold cross-validation.\n",
    "    Trains classifier on k-1 folds and tests on the remaining fold.\n",
    "    Calculates the k-fold cross-validation performance metric for classifier\n",
    "    by averaging the performance across folds.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf    -- classifier (instance of SVC)\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score   -- float, average cross-validation performance across k folds\n",
    "    \"\"\"\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2b: compute average cross-validation performance \n",
    "    score2 = []\n",
    "    for train_index, test_index in kf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        #print(\"after clf.fit\")\n",
    "        y_pred_cv = clf.decision_function(X_test)\n",
    "        score2.append(performance(y_true = y_test, y_pred = y_pred_cv, metric=metric))\n",
    "\n",
    "    score2 = np.mean(score2)\n",
    "\n",
    "    return score2\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_linear(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameter of a linear-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameter that 'maximize' the average k-fold CV performance.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X      -- numpy array of shape (n,d), feature vectors\n",
    "                    n = number of examples\n",
    "                    d = number of features\n",
    "        y      -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        C -- float, optimal parameter value for linear-kernel SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    print ('Linear SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    C_range = 10.0 ** np.arange(-3, 3)\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 2c: select optimal hyperparameter using cross-validation\n",
    "    c = [10** -3, 10** -2, 10**-1, 10**0, 10**1, 10**2]\n",
    "    score1 = []\n",
    "    for i in c:\n",
    "        model = SVC(kernel=\"linear\", C=i)\n",
    "        #print(\"after svc\")\n",
    "        score1.append(cv_performance(X=X, y=y, kf=kf, clf=model, metric=metric))\n",
    "\n",
    "    best_index = score1.index(max(score1))\n",
    "\n",
    "    #print(\"score1\", score1)\n",
    "    \n",
    "    return score1[best_index], c[best_index]\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def select_param_rbf(X, y, kf, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Sweeps different settings for the hyperparameters of an RBF-kernel SVM,\n",
    "    calculating the k-fold CV performance for each setting, then selecting the\n",
    "    hyperparameters that 'maximize' the average k-fold CV performance.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        X       -- numpy array of shape (n,d), feature vectors\n",
    "                     n = number of examples\n",
    "                     d = number of features\n",
    "        y       -- numpy array of shape (n,), binary labels {1,-1}\n",
    "        kf     -- cross_validation.KFold or cross_validation.StratifiedKFold\n",
    "        metric  -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        gamma, C -- tuple of floats, optimal parameter values for an RBF-kernel SVM\n",
    "    \"\"\"\n",
    "    \n",
    "    print ('RBF SVM Hyperparameter Selection based on ' + str(metric) + ':')\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 3b: create grid, then select optimal hyperparameters using cross-validation\n",
    "    c = [10** -3, 10** -2, 10**-1, 10**0, 10**1, 10**2]\n",
    "    r = [10** -3, 10** -2, 10**-1, 10**0, 10**1, 10**2]\n",
    "\n",
    "    #c = [10** -3, 10** -2 ]\n",
    "    #r = [10** -3, 10** -2]\n",
    "    score2 = []\n",
    "    best = 0\n",
    "    for i in c:\n",
    "        for j in r:\n",
    "            model = SVC(kernel=\"rbf\", C=i, gamma=j)\n",
    "            score2.append(tuple((cv_performance(X=X, y=y, kf=kf, clf=model, metric=metric), i, j)))\n",
    "\n",
    "    best = max(score2)\n",
    "    #print(\"score2\", score2)\n",
    "    #print(\"Max\", best)\n",
    "    # return gamma, c\n",
    "    return best[0], best[1], best[2]\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "def performance_test(clf, X, y, metric=\"accuracy\"):\n",
    "    \"\"\"\n",
    "    Estimates the performance of the classifier using the 95% CI.\n",
    "    \n",
    "    Parameters\n",
    "    --------------------\n",
    "        clf          -- classifier (instance of SVC)\n",
    "                          [already fit to data]\n",
    "        X            -- numpy array of shape (n,d), feature vectors of test set\n",
    "                          n = number of examples\n",
    "                          d = number of features\n",
    "        y            -- numpy array of shape (n,), binary labels {1,-1} of test set\n",
    "        metric       -- string, option used to select performance measure\n",
    "    \n",
    "    Returns\n",
    "    --------------------\n",
    "        score        -- float, classifier performance\n",
    "    \"\"\"\n",
    "\n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 4b: return performance on test data by first computing predictions and then calling performance\n",
    "    #cv_performance()\n",
    "\n",
    "    y_pred = clf.decision_function(X)\n",
    "    score = performance(y_true=y, y_pred=y_pred, metric= metric)\n",
    "\n",
    "    return score\n",
    "    ### ========== TODO : END ========== ###\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# main\n",
    "######################################################################\n",
    " \n",
    "def main() :\n",
    "    np.random.seed(1234)\n",
    "    \n",
    "    # read the tweets and its labels   \n",
    "    dictionary = extract_dictionary('../data/tweets.txt')\n",
    "    X = extract_feature_vectors('../data/tweets.txt', dictionary)\n",
    "\n",
    "    y = read_vector_file('../data/labels.txt')\n",
    "    \n",
    "    metric_list = [\"accuracy\", \"f1_score\", \"auroc\", \"precision\", \"sensitivity\", \"specificity\"]\n",
    "    \n",
    "    ### ========== TODO : START ========== ###\n",
    "    # part 1c: split data into training (training + cross-validation) and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/9)\n",
    "\n",
    "\n",
    "    # part 2b: create stratified folds (5-fold CV)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "    # skf.split(X_train, y_train)\n",
    "   \n",
    "    \n",
    "    # part 2d: for each metric, select optimal hyperparameter for linear-kernel SVM using CV\n",
    "    best = []\n",
    "    best.append(\"Best Score\")\n",
    "    best_c = []\n",
    "    best_c.append(\"Best C\")\n",
    "    for m in metric_list:\n",
    "        #result, c = select_param_linear(X_train, y_train, kf = skf.split(X_train, y_train), metric=m)\n",
    "        result, c = select_param_linear(X_train, y_train, kf = skf, metric=m)\n",
    "        best.append(result)\n",
    "        best_c.append(c)\n",
    "\n",
    "    table = [ metric_list, best, best_c]\n",
    "    print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
    "    print(\"From the result we can see that C=1 is the best for the model.\")\n",
    "    print(\"\")\n",
    "\n",
    "    # part 3a: How does gamma affect generalization error?\n",
    "    print(\"part 3a: The gamma parameter defines how far the influence of a single training example reaches, \")\n",
    "    print(\"with low values meaning ‘far’ and high values meaning ‘close’.\")\n",
    "    print(\"\")\n",
    "\n",
    "    # part 3b: what kind of grid you used and why\n",
    "    print(\"part 3b: I check every possible combination of C and gamma.\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # part 3c: for each metric, select optimal hyperparameter for RBF-SVM using CV\n",
    "    print(\"part 3c: for each metric, select optimal hyperparameter for RBF-SVM using CV\")\n",
    "    c_list = []\n",
    "    c_list.append(\"Best c\")\n",
    "\n",
    "    g_list = []\n",
    "    g_list.append(\"Gamma\")\n",
    "\n",
    "    s_list = []\n",
    "    s_list.append(\"Score\")\n",
    "    for m in metric_list:\n",
    "        #best_c, best_gamma, best_score = select_param_rbf(X_train, y_train, kf = skf.split(X_train, y_train), metric=m)\n",
    "        best_score, best_c, best_gamma = select_param_rbf(X_train, y_train, kf = skf, metric=m)\n",
    "        c_list.append(round(best_c,2))\n",
    "        g_list.append(round(best_gamma,2))\n",
    "        s_list.append(round(best_score,2))\n",
    "    \n",
    "    table = [ metric_list, s_list, c_list, g_list]\n",
    "   \n",
    "    print(\"c_list\", c_list)\n",
    "    print(\"g_list\", g_list)\n",
    "    print(\"s_list\", s_list)\n",
    " \n",
    "\n",
    "    print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))\n",
    "\n",
    "    print(\"Compare with Linear-Kernel SVM, we can see that in RBF-kernel SVM method c = 10 are most likely\")\n",
    "    print(\"to be the best, but in Linear-Kernel SVM c = 1 are the best.\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # part 4a: train linear- and RBF-kernel SVMs with selected hyperparameters\n",
    "\n",
    "    print(\"part 4a: train linear- and RBF-kernel SVMs with selected hyperparameters\")\n",
    "    print(\"Based on the previous testing result, I choose c=1 for Linear Kernel SVM and c = 10, gamma = 0.01\")\n",
    "    print(\"for the RBF-kernel method.\")\n",
    "    print(\"\")\n",
    "\n",
    "    model3 = SVC(kernel=\"linear\", C=1)\n",
    "    model3.fit(X_train, y_train)\n",
    "    result3 = performance_test(model3, X_test, y_test, metric=\"f1_score\")\n",
    "    \n",
    "\n",
    "    model4 = SVC(kernel=\"rbf\", C=10, gamma=0.01)\n",
    "    model4.fit(X_train, y_train)\n",
    "    result4 = performance_test(model4, X_test, y_test, metric=\"f1_score\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #model2 = SVC(kernel=\"rbf\", C=i, gamma=j)\n",
    "\n",
    "    # part 4c: report performance on test data\n",
    "    print(\"part 4c: report performance on test data\")\n",
    "    print(\"The performance of Linear-Kernel SVM is\", result3, \"which is good enough for this model.\")\n",
    "    print(\"The performance of RBF model\", result4, \"which is also good enough for this model. \")\n",
    "    print(\"Base on the result, Linear-Kernel SVM is better than RBF model. \")\n",
    "    ### ========== TODO : END ========== ###\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\" :\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a97895d194c721a7a9a46f13adaf5f2f2a3fc5ecb1e0d3b094b9a579f28670ef"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
